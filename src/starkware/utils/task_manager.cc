// Copyright 2023 StarkWare Industries Ltd.
//
// Licensed under the Apache License, Version 2.0 (the "License").
// You may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// https://www.starkware.co/open-source-license/
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions
// and limitations under the License.

#include "starkware/utils/task_manager.h"

#include <algorithm>
#include <utility>

#include "glog/logging.h"

#include "starkware/math/math.h"

#ifdef __EMSCRIPTEN__
DEFINE_uint32(n_threads, 1, "Number of threads to use.");
#else
DEFINE_uint32(n_threads, std::thread::hardware_concurrency(), "Number of threads to use.");
#endif

namespace starkware {

TaskManager::TaskManager(const size_t n_threads) {
  ASSERT_RELEASE(n_threads > 0, "Number of threads must be at least 1");
#ifdef __EMSCRIPTEN__
  ASSERT_RELEASE(n_threads == 1, "Multi-threading is not yet supported in WebAssembly");
#endif

  SetWorkerIdForCurrentThread(0);
  for (size_t i = 0; i < n_threads - 1; i++) {
    workers_.emplace_back([this, id = i + 1]() {
      SetWorkerIdForCurrentThread(id);
      TaskRunner(&new_pending_task_, &continue_running_);
    });
  }
}

TaskManager::~TaskManager() {
  {
    std::unique_lock<std::mutex> cv_lock(mutex_);
    // LOG rather than assert, because we don't want to throw in a destructor.
    LOG_IF(ERROR, !tasks_.empty()) << "Threadpool destructor called while tasks are pending";
    continue_running_ = 0;
    new_pending_task_.NotifyAll();
    task_group_finished_.NotifyAll();
  }
  for (auto& t : workers_) {
    t.join();
  }
}
void TaskManager::InitSingleton() { singleton = new TaskManager(FLAGS_n_threads); }

TaskManager TaskManager::CreateInstanceForTesting(size_t n_threads) {
  return TaskManager(n_threads);
}

void TaskManager::TaskRunner(CvWithWaitersCount* cv, const size_t* siblings_counter) {
  struct sched_param params {};
#ifndef __APPLE__
  int ret = sched_setscheduler(0, SCHED_BATCH, &params);
  ASSERT_RELEASE(ret == 0, "Filed to set scheduling policy.");

  int policy = sched_getscheduler(0);
  ASSERT_RELEASE(policy == SCHED_BATCH, "the scheduling policy was not set properly.");
#endif
  for (;;) {
    std::unique_lock<std::mutex> lock(mutex_);
    while (*siblings_counter > 0 && tasks_.empty()) {
      cv->Wait(&lock);
    }

    if (*siblings_counter == 0) {
      // There are two ways of reaching this point:
      // 1. The destructor was called and siblings_counter == &continue_running_.
      // 2. We are in the context of a ParallelFor call and all the tasks generated by
      // that call have completed.
      // Note that in case 2 tasks_ is not necessarily empty, it might contain tasks from
      // a diffrent ParallelFor call.

      return;
    }

    const auto task = std::move(tasks_.back());
    tasks_.pop_back();
    if (!tasks_.empty()) {
      // There are two events where would like to wake up a sleeping thread:
      // 1. There is a new task waiting to be executed.
      // 2. The group of tasks that the thread is waiting for has finished.
      // Ideally we would have liked a thread to block on both events, but there is no
      // wait for multiple events in the standard library.
      // As a workaround, threads sleep on either event 1 or event 2 and when we have a pending task
      // we check both events for waiting threads.
      if (!new_pending_task_.TryNotify()) {
        task_group_finished_.TryNotify();
      }
    }
    lock.unlock();
    task();
  }
}

/*
  Returns the equivelent of std::min(start_idx + chunk_size, end_idx) while taking care of
  uint64_t overflow in start_idx + chunk_size.
*/
static uint64_t GetTaskEndIdx(uint64_t start_idx, uint64_t chunk_size, uint64_t end_idx) {
  uint64_t res = start_idx + chunk_size;
  if (res < start_idx || res > end_idx) {
    res = end_idx;
  }
  return res;
}

void TaskManager::ParallelFor(
    uint64_t start_idx, uint64_t end_idx, const std::function<void(const TaskInfo&)>& func,
    uint64_t max_chunk_size_for_lambda, uint64_t min_work_chunk) {
  uint64_t split_size = std::max(
      min_work_chunk, DivCeil(end_idx - start_idx, kTaskRedudencyFactor * GetNumThreads()));

  size_t siblings_counter = 0;
  std::exception_ptr eptr = nullptr;

  if (start_idx + 1 == end_idx) {
    struct TaskInfo info {};
    info.start_idx = start_idx;
    info.end_idx = end_idx;
    func(info);
    return;
  }

  std::unique_lock<std::mutex> cv_lock(mutex_);

  for (uint64_t task_end_idx, task_idx = start_idx; task_idx < end_idx; task_idx = task_end_idx) {
    ++siblings_counter;

    task_end_idx = GetTaskEndIdx(task_idx, split_size, end_idx);

    uint64_t chunk_size = max_chunk_size_for_lambda;
    tasks_.emplace_back(
        [this, &func, task_idx, task_end_idx, chunk_size, &siblings_counter, &eptr] {
          std::exception_ptr exception = nullptr;
          struct TaskInfo info {};
          uint64_t i;
          for (i = task_idx; i < task_end_idx; i = info.end_idx) {
            info.start_idx = i;
            info.end_idx = GetTaskEndIdx(i, chunk_size, task_end_idx);

            try {
              func(info);
            } catch (...) {
              exception = std::current_exception();
              break;
            }
          }

          std::unique_lock<std::mutex> lock(mutex_);
          if (exception != nullptr && eptr == nullptr) {
            eptr = exception;
          }

          --siblings_counter;
          if (siblings_counter == 0) {
            task_group_finished_.NotifyAll();
          }
        });
  }

  cv_lock.unlock();
  TaskRunner(&task_group_finished_, &siblings_counter);

  // When we arrive to this point, all the tasks that were spawned above have finished.
  // It is safe to read eptr without a lock.
  ASSERT_RELEASE(siblings_counter == 0, "TaskRunner returned before all siblings completed");

  if (eptr != nullptr) {
    std::rethrow_exception(eptr);
  }
}

gsl::owner<TaskManager*> TaskManager::singleton;
std::once_flag TaskManager::singleton_flag;
#ifndef __EMSCRIPTEN__
thread_local size_t TaskManager::worker_id;
#else
size_t TaskManager::worker_id;
#endif

}  // namespace starkware
